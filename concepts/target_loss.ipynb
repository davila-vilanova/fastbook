{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting prediction value based on target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have some activations (for example, these are some random normalized activations (predictions) from a model with 2 outputs applied to 5 items):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.6734,  0.2576],\n",
       "         [ 0.4689,  0.4607],\n",
       "         [-2.2457, -0.3727],\n",
       "         [ 4.4164, -1.2760],\n",
       "         [ 0.9233,  0.5347]]),\n",
       " tensor([[0.6025, 0.3975],\n",
       "         [0.5021, 0.4979],\n",
       "         [0.1332, 0.8668],\n",
       "         [0.9966, 0.0034],\n",
       "         [0.5959, 0.4041]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cnt_outputs = 2\n",
    "cnt_items = 5\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "logits = torch.randn(cnt_items, cnt_outputs) * 2\n",
    "activations = torch.softmax(logits, dim=1)\n",
    "logits, activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have some targets as values between 0 and `cnt_outputs-1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.randint(0, cnt_outputs, (cnt_items,))\n",
    "targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to use tensor indexing to extract the likelihood assigned to the target corresponding to each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3975, 0.5021, 0.8668, 0.0034, 0.4041])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel0 = activations[range(cnt_items), targets]\n",
    "sel0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't get exactly what is allowed as tensor indexing syntax -- above it seems that we have:\n",
    "\n",
    "- a range, which is a quasi-vector, spanning the count of items, which matches the first dimension of the predictions\n",
    "- a vector of the same length with the right target for each row\n",
    "\n",
    "And we're using the value $n$ from the range and $m$ from the targets to get the right $(n, m)$ from the predictions 2D tensor, which is indeed like indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nll_loss` does the same as the advanced indexing above, but flips the sign to negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3975, -0.5021, -0.8668, -0.0034, -0.4041])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel1 = torch.nn.functional.nll_loss(\n",
    "    activations,\n",
    "    targets,\n",
    "    reduction=\"none\",  # so it doesn't take the mean of the output\n",
    ")\n",
    "assert torch.allclose(sel0, -sel1), (\n",
    "    \"Expecting target-based indexing and nll_loss to produce the same result but of inverse sign\"\n",
    ")\n",
    "\n",
    "sel1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `nll_loss` doesn't take the log, it assumes you already took the log of the softmax. And there's a function called `log_softmax` that combines `log` and `softmax` in a fast _and accurate_ way. That combo (softmax + log) is called _cross-entropy loss_.\n",
    "\n",
    "And PyTorch's `nn.CrossEntropyLoss` (or`F.cross_entropy`) is a combination of `log_softmax` and then `nll_loss`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what a logarithm is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e, log\n",
    "\n",
    "base = e\n",
    "a = 5\n",
    "y = base**a\n",
    "a == log(y, base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how as the probability gets smaller the log gets much smaller. It's amplifying the difference. So even if 0.99 and 0.999 are pretty close, the latter is 10 times as confident, and the log tries to reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import plot_function\n",
    "\n",
    "plot_function(torch.log, min=0, max=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following relationship is key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 12  # we already have an a, need a b for the thing below\n",
    "log(a * b, base) == log(a, base) + log(b, base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that logarithms increase linearly when the underlying signal increases exponentially or multiplicatively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

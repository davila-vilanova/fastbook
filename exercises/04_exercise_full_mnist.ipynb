{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "\n",
    "fastbook.setup_book()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "gpu = torch.device(\"mps\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "device = gpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from fastai.data.external import URLs, untar_data\n",
    "\n",
    "path = untar_data(URLs.MNIST)\n",
    "print(f\"MNIST data downloaded to {path}\")\n",
    "# Path.BASE_PATH = path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define how to access images from training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import List, Literal\n",
    "\n",
    "Digit = Literal[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "\n",
    "class DataSplit(Enum):\n",
    "    TRAINING = \"training\"\n",
    "    TESTING = \"testing\"\n",
    "\n",
    "\n",
    "def get_digit_file_paths(digit: Digit, datasplit: DataSplit) -> List[Path]:\n",
    "    return list((path / datasplit.value / str(digit)).ls().sorted())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peek into the images before turning them into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "im4 = Image.open(get_digit_file_paths(4, DataSplit.TRAINING)[0])\n",
    "print(f\"Image shape is {im4.shape}\")\n",
    "im4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "\n",
    "from fastai.torch_core import tensor\n",
    "\n",
    "\n",
    "@cache\n",
    "def get_digit_tensors(digit: Digit, datasplit: DataSplit) -> List[tensor]:\n",
    "    \"\"\"Gets all tensor images for the given digit in the specified DataSet.\"\"\"\n",
    "    return [\n",
    "        tensor(Image.open(path), device=device)\n",
    "        for path in get_digit_file_paths(digit, datasplit)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_core import show_image\n",
    "\n",
    "show_image(get_digit_tensors(3, DataSplit.TRAINING)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring tensor shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final, Tuple\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "IMAGE_SHAPE: Final[Tuple[int, int]] = (28, 28)\n",
    "\n",
    "\n",
    "class EmptyInputError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TensorShapeError(Exception):\n",
    "    def __init__(self, message: str, offending_tensor: Tensor):\n",
    "        self.offending_tensor = offending_tensor\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "def ensure_shape(tensor: Tensor, expected: Tuple[int, ...]) -> tensor:\n",
    "    \"\"\"Validates tensor shape matches expected dimensions. Use -1 for 'any'.\"\"\"\n",
    "    if len(tensor.shape) != len(expected):\n",
    "        raise TensorShapeError(\n",
    "            \"Tensor shape and expected shape must have the \"\n",
    "            \"same number of dimensions. \"\n",
    "            f\"Got {len(tensor.shape)} and {len(expected)}.\",\n",
    "            tensor,\n",
    "        )\n",
    "    for actual, expected in zip(tensor.shape, expected):\n",
    "        if expected == -1:\n",
    "            continue\n",
    "        if actual != expected:\n",
    "            raise TensorShapeError(\n",
    "                f\"Expected shape {expected}, got {tensor.shape}\", tensor\n",
    "            )\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting pixel values to 0...1 floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pixel_data(input: Tensor) -> Tensor:\n",
    "    \"\"\"Probably not a good name. Takes a tensor of image data with pixel values\n",
    "    between 0 and 255 and returns a tensor with float values between 0 and 1\"\"\"\n",
    "    return input.float() / 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking tensor images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "\n",
    "def stack_image_tensors(image_tensors: Sequence[tensor]) -> Tensor:\n",
    "    \"\"\"Takes a sequence of MNIST digit image tensors and returns a single tensor\n",
    "    whose first dimension refers to each of the images.\"\"\"\n",
    "\n",
    "    if len(image_tensors) == 0:\n",
    "        raise EmptyInputError(\n",
    "            \"The input sequence of tensors must have at least one element.\"\n",
    "        )\n",
    "\n",
    "    for t in image_tensors:\n",
    "        ensure_shape(t, IMAGE_SHAPE)\n",
    "\n",
    "    # It may be silly to wrap this in a function, but doing so adds some semantics\n",
    "    # and checks that the input has the expected shape.\n",
    "    stacked = torch.stack(image_tensors)\n",
    "\n",
    "    # This should never raise, so it just documents what I'm expecting.\n",
    "    ensure_shape(stacked, (-1,) + IMAGE_SHAPE)\n",
    "    return stacked\n",
    "\n",
    "\n",
    "@cache\n",
    "def get_stacked_preprocessed_digits(digit: Digit, datasplit: DataSplit) -> Tensor:\n",
    "    \"\"\"Gets all images for a given digit in the specified datasplit, stacked in a\n",
    "    single tensor and normalized.\"\"\"\n",
    "    print(\n",
    "        f\"get_stacked_preprocessed_digits is running for digit: {digit}, split: {datasplit}\"\n",
    "    )\n",
    "    return normalize_pixel_data(\n",
    "        stack_image_tensors(get_digit_tensors(digit, datasplit))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "def test_stacked_threes():\n",
    "    stacked_training_threes = stack_image_tensors(\n",
    "        get_digit_tensors(3, DataSplit.TRAINING)\n",
    "    )\n",
    "    # A stacked tensor of threes is rank-3 with ~6k images, each of which is 28 x 28 pixels.\n",
    "    assert stacked_training_threes.shape == (6131,) + IMAGE_SHAPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stacked_threes()\n",
    "show_image(stack_image_tensors(get_digit_tensors(3, DataSplit.TRAINING))[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Pixel Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the mean digit images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_image(stacked_images: Tensor) -> Tensor:\n",
    "    \"\"\"Calculates the \"ideal\" digit image, composed of the mean pixel values from\n",
    "    each of the images in the first (0) dimension in the `stacked_images` argument.\"\"\"\n",
    "    ensure_shape(stacked_images, (-1,) + IMAGE_SHAPE)\n",
    "    mean = torch.mean(\n",
    "        stacked_images, 0, dtype=torch.float32\n",
    "    )  # why can't pytorch infer the dtype?\n",
    "    ensure_shape(mean, IMAGE_SHAPE)\n",
    "    return mean\n",
    "\n",
    "\n",
    "@cache\n",
    "def mean_digit_image(digit: Digit, datasplit: DataSplit) -> Tensor:\n",
    "    \"\"\"Gets an averaged image for a given digit in a specified DataSet.\"\"\"\n",
    "    return mean_image(get_stacked_preprocessed_digits(digit, datasplit))\n",
    "\n",
    "\n",
    "def mean_digit_images(datasplit: DataSplit) -> Tensor:\n",
    "    \"\"\"Returns the averaged image for each digit for the specified DataSet.\"\"\"\n",
    "    return torch.stack([mean_digit_image(digit, datasplit) for digit in range(0, 10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for digit in range(0, 10):\n",
    "    show_image(mean_digit_images(DataSplit.TRAINING)[digit])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating how close a given digit image is from the \"ideal\", or mean image for that digit\n",
    "\n",
    "* Mean absolute difference, or L1 norm, is the mean of the absolute value of the differences between pixels.\n",
    "* Root mean square error, RMSE, or L2 norm, takes the mean of the squares of the differences and then the square root of that mean.\n",
    "  * this one penalizes larger mistakes more and smaller mistakes less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_norm(candidate: Tensor, ideal: Tensor) -> Tensor:\n",
    "    \"\"\"Calculates the mean of the absolute value of the differences between\n",
    "    pixels of the candidate and the \"ideal\" image tensors.\n",
    "    Returns a scalar wrapped in a rank-0 tensor.\"\"\"\n",
    "    ensure_shape(candidate, IMAGE_SHAPE)\n",
    "    ensure_shape(ideal, IMAGE_SHAPE)\n",
    "    result = (candidate - ideal).abs().mean()\n",
    "    ensure_shape(result, ())\n",
    "    return result\n",
    "\n",
    "\n",
    "def l2_norm(candidate: Tensor, ideal: Tensor) -> Tensor:\n",
    "    \"\"\"Calculates the root of the mean of the squares of the differences\n",
    "    (that is diff_tensor -> square -> mean_scalar -> sqrt)\n",
    "    between pixels of the candidate and the \"ideal\" image tensors.\n",
    "    Returns a scalar wrapped in a rank-0 tensor.\"\"\"\n",
    "    ensure_shape(candidate, IMAGE_SHAPE)\n",
    "    ensure_shape(ideal, IMAGE_SHAPE)\n",
    "    result = ((candidate - ideal) ** 2).mean().sqrt()\n",
    "    ensure_shape(result, ())\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    norm_function(\n",
    "        get_stacked_preprocessed_digits(3, DataSplit.TRAINING)[0],\n",
    "        mean_digit_image(3, DataSplit.TRAINING),\n",
    "    )\n",
    "    for norm_function in [l1_norm, l2_norm]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Consider removing check_shape\n",
    "def mnist_distance(\n",
    "    candidate_s: Tensor, ideal: Tensor, check_shape: bool = True\n",
    ") -> Tensor:\n",
    "    \"\"\"Calculates the \"distance(s)\" between a candidate image tensor OR a tensor of\n",
    "    candidate image tensors and an \"ideal\" image tensor.\n",
    "    If passed a single candidate it will return a scalar wrapped in a rank-0 tensor,\n",
    "    and if passed a tensor of candidates it will return a rank-1 tensor with the\n",
    "    corresponding distances (by broadcasting the \"ideal\" tensor.)\n",
    "    The \"distance(s)\" is/are calculated by taking the absolute difference of the pixel\n",
    "    values of candidate-ideal image pairs and calculating the mean of those pixel\n",
    "    differences, resulting in a scalar for each of the candidates.\"\"\"\n",
    "    if check_shape:\n",
    "        if candidate_s.ndim == 2:  # single image\n",
    "            ensure_shape(candidate_s, IMAGE_SHAPE)\n",
    "        else:  # many images\n",
    "            ensure_shape(candidate_s, (-1,) + IMAGE_SHAPE)\n",
    "\n",
    "    distance_s = (candidate_s - ideal).abs().mean((-1, -2))\n",
    "    # \"The tuple (-1,-2) represents a range of axes. In Python, -1 refers to the\n",
    "    # last element, and -2 refers to the second-to-last. So in this case, this\n",
    "    # tells PyTorch that we want to take the mean ranging over the values indexed\n",
    "    # by the last two axes of the tensor. The last two axes are the horizontal\n",
    "    # and vertical dimensions of an image.\"\n",
    "\n",
    "    if check_shape:\n",
    "        if candidate_s.ndim == 2:\n",
    "            # The output is a rank-0 tensor wrapping a single distance value for\n",
    "            # one image.\n",
    "            ensure_shape(distance_s, ())\n",
    "        else:\n",
    "            # The output vector has as many distance values as there are images in\n",
    "            # the input tensor.\n",
    "            ensure_shape(distance_s, (candidate_s.shape[0],))\n",
    "    return distance_s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between a three and the ideal three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_distance = mnist_distance(\n",
    "    get_stacked_preprocessed_digits(3, DataSplit.TRAINING)[0],\n",
    "    mean_digit_image(3, DataSplit.TRAINING),\n",
    ")\n",
    "print(f\"shape: {_distance.shape}\")\n",
    "_distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distances between each of the threes and the ideal three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_distances = mnist_distance(\n",
    "    get_stacked_preprocessed_digits(3, DataSplit.TRAINING),\n",
    "    mean_digit_image(3, DataSplit.TRAINING),\n",
    ")\n",
    "print(f\"shape: {_distances.shape}\")\n",
    "_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between a concrete three and each of the ideal digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_distances = mnist_distance(\n",
    "    get_stacked_preprocessed_digits(3, DataSplit.TRAINING)[0],\n",
    "    mean_digit_images(DataSplit.TRAINING),\n",
    "    check_shape=False,\n",
    ")\n",
    "print(f\"shape: {_distances.shape}\")\n",
    "_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between each of the threes and each of the ideal digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to use this later so I'll wrap it in a function\n",
    "def calculate_distances(\n",
    "    digit: Digit, from_datasplit: DataSplit, to_ideal_digits_datasplit: DataSplit\n",
    ") -> Tensor:\n",
    "    expected_digit_count = len(get_digit_file_paths(digit, from_datasplit))\n",
    "    shaped_concrete_digits = get_stacked_preprocessed_digits(\n",
    "        digit, from_datasplit\n",
    "    ).unsqueeze(1)\n",
    "    ensure_shape(\n",
    "        shaped_concrete_digits, (expected_digit_count, 1) + IMAGE_SHAPE\n",
    "    )  # [~6K, 1, 28, 28]\n",
    "    shaped_means = mean_digit_images(DataSplit.TRAINING).unsqueeze(0)\n",
    "    ensure_shape(shaped_means, (1, 10, 28, 28))\n",
    "    distances = mnist_distance(shaped_concrete_digits, shaped_means, check_shape=False)\n",
    "    ensure_shape(distances, (expected_digit_count, 10))  # [~6K, 10]\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_distances = calculate_distances(\n",
    "    digit=3,\n",
    "    from_datasplit=DataSplit.TRAINING,\n",
    "    to_ideal_digits_datasplit=DataSplit.TRAINING,\n",
    ")\n",
    "print(f\"shape: {_distances.shape}\")\n",
    "_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between each concrete digit and each of the ideal digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will use a loop instead of broadcasting because the training sets of different\n",
    "# digits have different lengths. Alternatively I could cap all of them to the\n",
    "# same length.\n",
    "[\n",
    "    calculate_distances(\n",
    "        digit,\n",
    "        from_datasplit=DataSplit.TRAINING,\n",
    "        to_ideal_digits_datasplit=DataSplit.TRAINING,\n",
    "    )\n",
    "    for digit in range(0, 10)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a concrete digit, which ideal digit is it closer to?\n",
    "def match(image: Tensor, ideals: Tensor) -> Digit:\n",
    "    ensure_shape(image, IMAGE_SHAPE)\n",
    "    ensure_shape(ideals, (10,) + IMAGE_SHAPE)\n",
    "    distances = mnist_distance(image, ideals, check_shape=False)\n",
    "    ensure_shape(distances, (10,))\n",
    "    # if there's more than one min this will produce the index of the first one\n",
    "    min_indexes = torch.argmin(distances)\n",
    "    min_index = min_indexes.item()\n",
    "    assert min_index in range(0, 10)\n",
    "    return min_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match(\n",
    "    image=get_stacked_preprocessed_digits(8, DataSplit.TRAINING)[3],\n",
    "    ideals=mean_digit_images(DataSplit.TRAINING),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a tensor of stacked digit images, which ideal digit is each closer to?\n",
    "def matches_for_images(images, ideals: Tensor) -> Tensor:\n",
    "    ensure_shape(images, (-1,) + IMAGE_SHAPE)\n",
    "    image_count = images.shape[0]\n",
    "    shaped_images = images.unsqueeze(1)\n",
    "    ensure_shape(shaped_images, (image_count, 1) + IMAGE_SHAPE)\n",
    "    ensure_shape(ideals, (10,) + IMAGE_SHAPE)\n",
    "    shaped_ideals = ideals.unsqueeze(0)\n",
    "    ensure_shape(shaped_ideals, (1, 10) + IMAGE_SHAPE)\n",
    "    distances = mnist_distance(shaped_images, shaped_ideals, check_shape=False)\n",
    "    ensure_shape(distances, (image_count, 10))\n",
    "    digits = torch.argmin(distances, dim=1)\n",
    "    ensure_shape(digits, (image_count,))\n",
    "    return digits\n",
    "\n",
    "\n",
    "def matches_for_digit(\n",
    "    digit: Digit,\n",
    "    from_datasplit: DataSplit,\n",
    "    ideals_datasplit: DataSplit,\n",
    ") -> Tensor:\n",
    "    return matches_for_images(\n",
    "        images=get_stacked_preprocessed_digits(digit, from_datasplit),\n",
    "        ideals=mean_digit_images(ideals_datasplit),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matches_for_digit(\n",
    "    3,\n",
    "    from_datasplit=DataSet.TRAINING,\n",
    "    ideals_datasplit=DataSet.TRAINING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_matches_3s = matches_for_images(\n",
    "    images=get_stacked_preprocessed_digits(3, DataSplit.TRAINING),\n",
    "    ideals=mean_digit_images(DataSplit.TRAINING),\n",
    ")\n",
    "_correct_3s = _matches_3s == 3\n",
    "_correct_3s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_correct_3s.float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    matches_for_digit(digit, DataSplit.TRAINING, DataSplit.TRAINING)\n",
    "    for digit in range(0, 10)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_accuracy(\n",
    "    digit: Digit,\n",
    "    from_datasplit: DataSplit,\n",
    "    ideals_datasplit: DataSplit,\n",
    ") -> Tensor:\n",
    "    matches = matches_for_digit(digit, from_datasplit, ideals_datasplit)\n",
    "    corrects = matches == digit\n",
    "    accuracy = corrects.float().mean()\n",
    "    ensure_shape(accuracy, ())  # returns a rank-0 tensor\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def baseline_accuracies(\n",
    "    from_datasplit: DataSplit,\n",
    "    ideals_datasplit: DataSplit,\n",
    ") -> List[float]:\n",
    "    # digits have different counts of samples\n",
    "    # so this is iterated instead of broadcasted\n",
    "    return [\n",
    "        baseline_accuracy(digit, from_datasplit, ideals_datasplit).item()\n",
    "        for digit in range(0, 10)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, calculate_accuracy_plain_labels in enumerate(\n",
    "    baseline_accuracies(DataSplit.TRAINING, DataSplit.TRAINING)\n",
    "):\n",
    "    print(f\"{i}: {calculate_accuracy_plain_labels:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import gv\n",
    "\n",
    "gv(\"\"\"\n",
    "init->predict->loss->gradient->step->stop\n",
    "step->predict[label=repeat]\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "def labeled_data(datasplit: DataSplit) -> Tuple[Tensor, Tensor]:\n",
    "    _all_digits = range(0, 10)\n",
    "    _stacked_digits = [\n",
    "        get_stacked_preprocessed_digits(digit, datasplit) for digit in _all_digits\n",
    "    ]\n",
    "    _lengths = [len(get_digit_file_paths(digit, datasplit)) for digit in _all_digits]\n",
    "\n",
    "    train_x = torch.cat(_stacked_digits).view(-1, IMAGE_SHAPE[0] * IMAGE_SHAPE[1])\n",
    "    train_y = tensor(\n",
    "        reduce(\n",
    "            lambda a, b: a + b, [[digit] * _lengths[digit] for digit in _all_digits]\n",
    "        ),\n",
    "        dtype=torch.int64,\n",
    "        device=device,\n",
    "    )\n",
    "    return (train_x, train_y)\n",
    "\n",
    "\n",
    "train_x, train_y = labeled_data(DataSplit.TRAINING)\n",
    "valid_x, valid_y = labeled_data(DataSplit.TESTING)\n",
    "print(f\"Training: {train_x.shape, train_y.shape}\")\n",
    "print(f\"Validation - testing: {valid_x.shape, valid_y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Dataset in PyTorch is required to return a tuple of (x,y) when indexed. \n",
    "It looks like this: $[(x_1,y_1), (x_2,y_2), ... (x_n,y_n)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dset = list(zip(train_x, train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = training_dset[0]\n",
    "batch_x.shape, batch_y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from torch import SymInt\n",
    "\n",
    "\n",
    "def init_params(shape: Union[int, SymInt], std: float = 1.0) -> Tensor:\n",
    "    return (torch.randn(shape, device=device) * std).requires_grad_()\n",
    "\n",
    "\n",
    "# TODO: I could add a different kind of param initializer here such as Kaiming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the book the model has to differentiate between 2 digits, so the model has a single output. 0.0 is used as some kind of threshold: any values greater than 0 represent a prediction for one digit, the others represent the other digit. For this 2-digit model, `train_y` is a vector of 1s for one digit and 0s for the other digit. (And then unsqueezed to a second dimension of size 1).\n",
    "\n",
    "0 is at the center of the model outputs distribution, then they switch it to 0.5 (that is, all values between 0 and 1) using **sigmoid**.\n",
    "\n",
    "I want to switch the model from having one input $y_1$ (probability of input being digit a, and $y_2=1-y_1$) to 10 outputs (originally I was thinking 9 so I would calculate the 10th as $1-p(others)$, but it makes more sense to use Softmax for 10 outputs, as the probs of the 10 should add to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_outputs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((IMAGE_SHAPE[0] * IMAGE_SHAPE[1], count_outputs))\n",
    "biases = init_params(count_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the full training data: {train_x.shape}\")\n",
    "print(f\"Shape of the weights: {weights.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model \"manually\"\n",
    "\n",
    "So I better understand the forward computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How I think the `batch @ weights + bias`, equation operates in a linear model with several outputs: it has 9 sets of parameters and 9 biases. So I'm imagining it's just that equation once per output probability, with the same input data, independently and in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it once for a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image = train_x[0]\n",
    "print(f\"Shape of the first image: {first_image.shape}\")\n",
    "\n",
    "# the T is transpose, flipping weights from [784, count_outputs] to [count_outputs, 784]\n",
    "transposed_weights = weights.T\n",
    "print(f\"Shape of transposed weights: {transposed_weights.shape}\")\n",
    "\n",
    "element_wise_product = first_image * transposed_weights\n",
    "print(f\"Shape of element-wise multiplication: {element_wise_product.shape}\")\n",
    "\n",
    "model_result = element_wise_product.sum() + biases\n",
    "print(f\"Shape of model result: {model_result.shape}\")\n",
    "print(f\"Model result: {model_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it once for a batch data using broadcasting (no loops!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = train_x  # the whole dataset -- it still feels instantaneous\n",
    "\n",
    "print(f\"Shape of batch: {input.shape}\")\n",
    "print(f\"Shape of weights: {weights.shape}\")\n",
    "print(f\"Shape of bias: {biases.shape}\")\n",
    "\n",
    "\n",
    "# Here is one of the two magic equations (the other one is the activation\n",
    "# function), equivalent to doing the run above but for each image in the batch\n",
    "# (in this case the full training set).\n",
    "model_result = input @ weights + biases\n",
    "\n",
    "print(f\"Shape of results: {model_result.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model wrapped in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapped in a class, together with its weights and biases, like models\n",
    "# in the pytorch nn package:\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from itertools import chain\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class Module(ABC):  # e.g. a model or layer\n",
    "    @abstractmethod\n",
    "    def run(self, input: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def params(self) -> Sequence[Tensor]:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.run(*args, **kwargs)\n",
    "\n",
    "    def verify_input(self, input: Tensor, in_features: int) -> Tensor:\n",
    "        return ensure_shape(input, (-1, in_features))\n",
    "\n",
    "    def verify_output(self, output: Tensor, out_features: int) -> Tensor:\n",
    "        return ensure_shape(output, (-1, out_features))\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        init_params_function: Callable[\n",
    "            [Union[int, SymInt]], Tensor\n",
    "        ],  # takes shape, returns tensor\n",
    "    ):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = init_params_function((in_features, out_features))\n",
    "        self.biases = init_params_function(out_features)\n",
    "\n",
    "    def run(self, input: Tensor) -> Tensor:\n",
    "        self.verify_input(input, self.in_features)\n",
    "        # Apply linear transformation: input (e.g. batch) @ weights + biases\n",
    "        linear = input @ self.weights + self.biases\n",
    "        return self.verify_output(\n",
    "            linear, self.out_features\n",
    "        )  # return unnormalized logits\n",
    "\n",
    "    def params(self) -> Sequence[Tensor]:\n",
    "        return [self.weights, self.biases]\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    def run(self, input: Tensor) -> Tensor:\n",
    "        return input.max(tensor(0.0, device=device))\n",
    "\n",
    "    def params(self) -> Sequence[Tensor]:\n",
    "        return []\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, submodules: Sequence[Module]):\n",
    "        self.submodules = submodules\n",
    "\n",
    "    def run(self, input: Tensor) -> Tensor:\n",
    "        # Call all the submodules in order, calling the first with the input,\n",
    "        # passing the output of each to the input of the next one, and returning\n",
    "        # the output of the last one\n",
    "        assert len(self.submodules) > 0\n",
    "        i = input\n",
    "        o: Tensor = None\n",
    "        for submodule in self.submodules:\n",
    "            if o is not None:\n",
    "                i = o\n",
    "            o = submodule(i)\n",
    "        return o\n",
    "\n",
    "    def params(self) -> Sequence[Tensor]:\n",
    "        return list(chain.from_iterable([sub.params() for sub in self.submodules]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a linear model to test some functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "_linear_model = Linear(\n",
    "    in_features=IMAGE_SHAPE[0] * IMAGE_SHAPE[1],\n",
    "    out_features=count_outputs,\n",
    "    init_params_function=init_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_linear_model(valid_x) is None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizers of logits to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_softmax(logits: Tensor) -> Tensor:\n",
    "    \"\"\"Normalizes logits applying softmax.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A tensor of predictions with the same dimensions as the input.\n",
    "        Predictions sum 1, and equal to the probability of each class match.\n",
    "    \"\"\"\n",
    "    return torch.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "def normalize_log_softmax(logits: Tensor) -> Tensor:\n",
    "    \"\"\"Normalizes logits applying log softmax.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A tensor of predictions with the same dimensions as the input.\n",
    "        Predictions sum 1, and equal to the probability of each class match.\n",
    "    \"\"\"\n",
    "    return torch.log_softmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to turn labels into targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def encode_one_hot_targets(labels: Tensor, num_classes=10) -> Tensor:\n",
    "    return F.one_hot(labels, num_classes).float()\n",
    "\n",
    "\n",
    "def pass_through_labels(labels: Tensor) -> Tensor:\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_y[0])\n",
    "print(encode_one_hot_targets(valid_y)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_plain_labels(preds: Tensor, labels: Tensor) -> float:\n",
    "    \"\"\"Calculates the accuracy of a matrix of predictions given a vector of labels.\n",
    "\n",
    "    Args:\n",
    "        preds (Tensor): A 2D matrix of image index to digit match likelihoods.\n",
    "        These can be logits or normalized. Just the maximum value of each row is used.\n",
    "        labels (Tensor): A 1D vector of labels corresponding to each image\n",
    "\n",
    "    Returns:\n",
    "        float: A scalar indicating the average accuracy\n",
    "    \"\"\"\n",
    "    assert preds.ndim == 2\n",
    "    ensure_shape(labels, (preds.shape[0],))\n",
    "    preds_as_digits = preds.argmax(dim=1)\n",
    "    correct = preds_as_digits == labels\n",
    "    return correct.float().mean().item()\n",
    "\n",
    "\n",
    "def validate_model_plain_labels(\n",
    "    model: Module, valid_x: Tensor, valid_y: Tensor\n",
    ") -> float:\n",
    "    return calculate_accuracy_plain_labels(model(valid_x), valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy_plain_labels(_linear_model(valid_x), valid_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to calculate the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the normalization step (such as Softmax) seems to conceptually fit better with the model, I see it being used in the loss function. I've extracted it into a normalizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_plain_labels(preds: Tensor, labels: Tensor) -> Tensor:\n",
    "    # preds must be normalized\n",
    "    ensure_shape(labels, (preds.shape[0],))\n",
    "    probs_right_guesses = torch.gather(\n",
    "        preds,\n",
    "        dim=1,\n",
    "        index=labels.unsqueeze(\n",
    "            dim=1\n",
    "        ),  # unsqueeze because \"Index tensor must have the same number of dimensions as input tensor\"\n",
    "    )\n",
    "    losses = 1 - probs_right_guesses\n",
    "    return ensure_shape(losses.sum(), ())\n",
    "\n",
    "\n",
    "def calculate_loss_one_host_targets(preds: Tensor, targets: Tensor) -> Tensor:\n",
    "    # preds must be normalized\n",
    "    assert preds.shape == targets.shape\n",
    "\n",
    "    # TODO is there a more efficient way to do this?\n",
    "    probability_right_guesses = torch.where(\n",
    "        targets == 1.0, preds, torch.zeros_like(preds)\n",
    "    )\n",
    "    losses = 1 - probability_right_guesses.sum(dim=1)\n",
    "    return losses.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two actually return the same value currently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_loss_plain_labels(normalize_softmax(_linear_model(train_x)), train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_loss_one_host_targets(\n",
    "    normalize_softmax(_linear_model(train_x)),\n",
    "    encode_one_hot_targets(train_y),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection that contains tuples of independent and dependent variables is a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(zip(train_x, train_y))\n",
    "valid_dataset = list(zip(valid_x, valid_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look somewhere in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_index = 40000\n",
    "show_image(train_dataset[_index][0].view((IMAGE_SHAPE)))\n",
    "train_dataset[_index][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we pass a dataset to a DataLoader we will get back many batches that are themselves tuples of tensors representing batches of independent and dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.load import DataLoader\n",
    "\n",
    "# A dataloader with a tiny batch size for playing around\n",
    "data = DataLoader(train_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import first\n",
    "\n",
    "first_training_batch = first(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training batch is a tuple of two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(first_training_batch, tuple), \"first_training_batch should be a tuple\"\n",
    "assert len(first_training_batch) == 2, \"first_training_batch should have length 2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element is a tensor of images and the second is a tensor of labels. Both have the same length, which corresponds to the `batch_size` passed to the `DataLoader` constructor above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "_images, _labels = first_training_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_labels.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image and label in the batch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into the first dataloader batch -> first image and label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(first(_images).view(IMAGE_SHAPE))\n",
    "first(_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass + calc grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns average loss\n",
    "def forward_pass_calc_grad(\n",
    "    batch_x: Tensor,\n",
    "    batch_y: Tensor,\n",
    "    model: Module,\n",
    "    normalize: Callable[[Tensor], Tensor],\n",
    "    loss_function: Callable[[Tensor, Tensor], Tensor],\n",
    ") -> float:\n",
    "    logits = model(batch_x)\n",
    "    preds = normalize(logits)\n",
    "    loss = loss_function(preds, batch_y)\n",
    "    loss.backward()\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, learning_rate: float, params: Sequence[Tensor]):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.params = params\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            p.data -= p.grad.data * self.learning_rate\n",
    "\n",
    "    def zero_grad(self):\n",
    "        # for p in self.params: p.grad = None\n",
    "        for p in self.params:\n",
    "            p.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cycle of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: untied end: training data with labels or targets\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    data: Union[DataLoader, Tuple[Tensor, Tensor]],\n",
    "    model,\n",
    "    *,\n",
    "    normalizer,\n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    epochs: int,\n",
    ") -> Generator[Tuple[int, float], None, None]:\n",
    "    def train_epoch(x: Tensor, y: Tensor):\n",
    "        forward_pass_calc_grad(\n",
    "            x,\n",
    "            y,\n",
    "            model,\n",
    "            normalizer,\n",
    "            loss_function,\n",
    "        )\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if isinstance(data, DataLoader):\n",
    "            for batch_x, batch_y in data:\n",
    "                train_epoch(batch_x, batch_y)\n",
    "        else:\n",
    "            train_epoch(data[0], data[1])\n",
    "\n",
    "        yield epoch, validate_model_plain_labels(model, valid_x, valid_y)\n",
    "        # TODO: will this have affected the parameter grads? Significantly?\n",
    "\n",
    "\n",
    "def plot_accuracies(accuracies: List[float]) -> None:\n",
    "    plt.plot(accuracies)\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Training Accuracy Over Batches\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_features = 20\n",
    "model = Sequential(\n",
    "    [\n",
    "        Linear(\n",
    "            in_features=IMAGE_SHAPE[0] * IMAGE_SHAPE[1],\n",
    "            out_features=intermediate_features,\n",
    "            init_params_function=init_params,\n",
    "        ),\n",
    "        ReLU(),\n",
    "        Linear(\n",
    "            in_features=intermediate_features,\n",
    "            out_features=count_outputs,\n",
    "            init_params_function=init_params,\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = (\n",
    "    train_x,\n",
    "    train_y,\n",
    ")  # DataLoader(train_dataset, batch_size=60000, shuffle=True, device=device)\n",
    "\n",
    "accuracies = list()\n",
    "\n",
    "for epoch, acc in train_model(\n",
    "    data,\n",
    "    model,\n",
    "    normalizer=normalize_softmax,\n",
    "    loss_function=calculate_loss_plain_labels,\n",
    "    optimizer=Optimizer(\n",
    "        learning_rate=1e-4,\n",
    "        params=model.params(),\n",
    "    ),\n",
    "    epochs=4000,\n",
    "):\n",
    "    elapsed = time.time() - start_time\n",
    "    accuracies.append(acc)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Time={elapsed:.2f}s, epoch: {epoch}, accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "\n",
    "def notify_complete(success=True):\n",
    "    \"\"\"Notify when cell execution completes\"\"\"\n",
    "    if platform.system() == \"Darwin\":  # macOS\n",
    "        os.system(\"afplay /System/Library/Sounds/Glass.aiff\")\n",
    "    else:  # Other platforms\n",
    "        print(\"\\a\")  # ASCII bell\n",
    "\n",
    "\n",
    "notify_complete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions after all of this:\n",
    "- How to choose the number of intermediate features?\n",
    "- How to play with LRs? Should the LR vary within a training run? With which shape?\n",
    "- How to saturate the GPU? How to profile bottlenecks?\n",
    "- Speaking of which, what's the deal with `DataLoader`? Why so slow?\n",
    "- How does batching help with learning (not only with speed)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

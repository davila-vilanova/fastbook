{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weird fastai magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "\n",
    "fastbook.setup_book()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "gpu = torch.device(\"mps\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "device = gpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST data downloaded to /Users/david/.fastai/data/mnist_png\n"
     ]
    }
   ],
   "source": [
    "from load_mnist_data import load_mnist_data\n",
    "\n",
    "mnist_path = load_mnist_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_stacked_preprocessed_digits is running for digit: 0, split: DataSplit.TRAINING\n",
      "get_stacked_preprocessed_digits is running for digit: 1, split: DataSplit.TRAINING\n",
      "get_stacked_preprocessed_digits is running for digit: 2, split: DataSplit.TRAINING\n",
      "get_stacked_preprocessed_digits is running for digit: 3, split: DataSplit.TRAINING\n",
      "get_stacked_preprocessed_digits is running for digit: 4, split: DataSplit.TRAINING\n",
      "get_stacked_preprocessed_digits is running for digit: 5, split: DataSplit.TRAINING\n",
      "get_stacked_preprocessed_digits is running for digit: 6, split: DataSplit.TRAINING\n",
      "get_stacked_preprocessed_digits is running for digit: 7, split: DataSplit.TRAINING\n",
      "get_stacked_preprocessed_digits is running for digit: 8, split: DataSplit.TRAINING\n",
      "get_stacked_preprocessed_digits is running for digit: 9, split: DataSplit.TRAINING\n",
      "get_stacked_preprocessed_digits is running for digit: 0, split: DataSplit.TESTING\n",
      "get_stacked_preprocessed_digits is running for digit: 1, split: DataSplit.TESTING\n",
      "get_stacked_preprocessed_digits is running for digit: 2, split: DataSplit.TESTING\n",
      "get_stacked_preprocessed_digits is running for digit: 3, split: DataSplit.TESTING\n",
      "get_stacked_preprocessed_digits is running for digit: 4, split: DataSplit.TESTING\n",
      "get_stacked_preprocessed_digits is running for digit: 5, split: DataSplit.TESTING\n",
      "get_stacked_preprocessed_digits is running for digit: 6, split: DataSplit.TESTING\n",
      "get_stacked_preprocessed_digits is running for digit: 7, split: DataSplit.TESTING\n",
      "get_stacked_preprocessed_digits is running for digit: 8, split: DataSplit.TESTING\n",
      "get_stacked_preprocessed_digits is running for digit: 9, split: DataSplit.TESTING\n"
     ]
    }
   ],
   "source": [
    "from load_mnist_data import IMAGE_SHAPE, DataSplit, labeled_data\n",
    "\n",
    "train_x, train_labels = labeled_data(mnist_path, DataSplit.TRAINING, device)\n",
    "valid_x, valid_labels = labeled_data(mnist_path, DataSplit.TESTING, device)\n",
    "\n",
    "assert train_x.shape == torch.Size([60_000, IMAGE_SHAPE[0] * IMAGE_SHAPE[1]]), (\n",
    "    \"train_x should be a matrix of 60,000 images, each a vector of flattened pixels\"\n",
    ")\n",
    "assert train_labels.shape == torch.Size([60_000]), (\n",
    "    \"train_labels should be a vector of 60,000 labels\"\n",
    ")\n",
    "\n",
    "assert valid_x.shape == torch.Size([10_000, IMAGE_SHAPE[0] * IMAGE_SHAPE[1]]), (\n",
    "    \"valid_x should be a matrix of 10,000 images, each a vector of flattened pixels\"\n",
    ")\n",
    "assert valid_labels.shape == torch.Size([10_000]), (\n",
    "    \"valid_labels should be a vector of 10,000 labels\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning labels into targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_mnist_data import NUM_CLASSES\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "\n",
    "def encode_one_hot_targets(labels: Tensor, num_classes=NUM_CLASSES) -> Tensor:\n",
    "    return one_hot(labels, num_classes).float()\n",
    "\n",
    "\n",
    "train_y = encode_one_hot_targets(train_labels)\n",
    "valid_y = encode_one_hot_targets(valid_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Dataset in PyTorch is required to return a tuple of (x,y) when indexed. \n",
    "It looks like this: $[(x_1,y_1), (x_2,y_2), ... (x_n,y_n)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(zip(train_x, train_y))\n",
    "valid_dataset = list(zip(valid_x, valid_y))\n",
    "\n",
    "assert len(train_dataset) == 60_000, \"training_dset should have 60,000 items\"\n",
    "\n",
    "image, label = train_dataset[0]\n",
    "assert image.shape == torch.Size(\n",
    "    [IMAGE_SHAPE[0] * IMAGE_SHAPE[1]]\n",
    ") and label.shape == torch.Size([NUM_CLASSES]), (\n",
    "    \"Each item in a dataset should be a tuple of an image and a hot-encoded label\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look somewhere in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], device='mps:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.basics import show_image\n",
    "\n",
    "image, label = train_dataset[40_000]\n",
    "\n",
    "show_image(image.view((IMAGE_SHAPE)))\n",
    "label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we pass a dataset to a DataLoader we will get back many batches that are themselves tuples of tensors representing collections of independent and dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import first\n",
    "from fastai.data.load import DataLoader\n",
    "\n",
    "# A dataloader with a tiny batch size for playing around\n",
    "toy_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training batch is a tuple of two tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_training_batch = first(toy_dataloader)\n",
    "\n",
    "assert isinstance(first_training_batch, tuple), \"first_training_batch should be a tuple\"\n",
    "assert len(first_training_batch) == 2, \"first_training_batch should have length 2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element is a tensor of images and the second is a tensor of labels. Both have the same length, which corresponds to the `batch_size` passed to the `DataLoader` constructor above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = first_training_batch\n",
    "assert images.shape == torch.Size([2, IMAGE_SHAPE[0] * IMAGE_SHAPE[1]])\n",
    "assert labels.shape == torch.Size([2, NUM_CLASSES])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image and label in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], device='mps:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(first(images).view(IMAGE_SHAPE))\n",
    "first(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a \"manual\" linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the book the model has to differentiate between 2 digits, so the model has a single output. 0.0 is used as some kind of threshold: any values greater than 0 represent a prediction for one digit, the others represent the other digit. For this 2-digit model, `train_y` is a vector of 1s for one digit and 0s for the other digit. (And then unsqueezed to a second dimension of size 1).\n",
    "\n",
    "0 is at the center of the model outputs distribution, then they switch it to 0.5 (that is, all values between 0 and 1) using **sigmoid**.\n",
    "\n",
    "I want to switch the model from having one input $y_1$ (probability of input being digit a, and $y_2=1-y_1$) to 10 outputs, each corresponding to the probability of\n",
    "the input matching a particular digit. The combined probabilities will be normalized to add to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import init_params\n",
    "\n",
    "weights = init_params((IMAGE_SHAPE[0] * IMAGE_SHAPE[1], NUM_CLASSES), device)\n",
    "biases = init_params(NUM_CLASSES, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The forward computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How I think the `batch @ weights + bias`, equation operates in a linear model with 10 outputs: it has 10 sets of parameters and 10 biases. So I'm imagining it's just that equation once per output, with the same input data, independently and in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it once for a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first image: torch.Size([784])\n",
      "Shape of transposed weights: torch.Size([10, 784])\n",
      "Shape of element-wise multiplication: torch.Size([10, 784])\n",
      "Shape of model result: torch.Size([10])\n",
      "Model result: tensor([5.0383, 6.0956, 6.5828, 5.7916, 7.5112, 6.7692, 5.8542, 6.7100, 6.4550, 6.5091], device='mps:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "first_image = train_x[0]\n",
    "print(f\"Shape of the first image: {first_image.shape}\")\n",
    "\n",
    "# the T is transpose, flipping weights from [784, count_outputs] to [count_outputs, 784]\n",
    "transposed_weights = weights.T\n",
    "print(f\"Shape of transposed weights: {transposed_weights.shape}\")\n",
    "\n",
    "element_wise_product = first_image * transposed_weights\n",
    "print(f\"Shape of element-wise multiplication: {element_wise_product.shape}\")\n",
    "\n",
    "model_result = element_wise_product.sum() + biases\n",
    "print(f\"Shape of model result: {model_result.shape}\")\n",
    "print(f\"Model result: {model_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it once for a batch data using broadcasting (no loops!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch: torch.Size([60000, 784])\n",
      "Shape of weights: torch.Size([784, 10])\n",
      "Shape of bias: torch.Size([10])\n",
      "Shape of results: torch.Size([60000, 10])\n"
     ]
    }
   ],
   "source": [
    "input = train_x  # the whole dataset -- it still feels instantaneous\n",
    "\n",
    "print(f\"Shape of batch: {input.shape}\")\n",
    "print(f\"Shape of weights: {weights.shape}\")\n",
    "print(f\"Shape of bias: {biases.shape}\")\n",
    "\n",
    "\n",
    "# Here is one of the two magic equations (the other one is the activation\n",
    "# function), equivalent to doing the run above but for each image in the batch\n",
    "# (in this case the full training set).\n",
    "model_result = input @ weights + biases\n",
    "\n",
    "print(f\"Shape of results: {model_result.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition about loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_core import tensor\n",
    "from torch.nn.functional import cross_entropy\n",
    "from training import calculate_loss\n",
    "\n",
    "_targets = tensor(\n",
    "    [\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 1],\n",
    "    ],\n",
    "    dtype=torch.float,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "_preds_worse = tensor(\n",
    "    [\n",
    "        [0.3, 0.5, 0.2],\n",
    "        [0.7, 0.1, 0.2],\n",
    "    ],\n",
    "    dtype=torch.float,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "_preds_better = tensor(\n",
    "    [\n",
    "        [0.4, 0.5, 0.1],\n",
    "        [0.6, 0.1, 0.3],\n",
    "    ],\n",
    "    dtype=torch.float,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "assert calculate_loss(_preds_worse, _targets) == 0.7 + 0.8, \"wrong loss calculation\"\n",
    "assert calculate_loss(_preds_better, _targets) == 0.6 + 0.7, \"wrong loss calculation\"\n",
    "assert calculate_loss(_preds_worse, _targets) > calculate_loss(\n",
    "    _preds_better, _targets\n",
    "), \"wrong loss comparison (calculate_loss)\"\n",
    "\n",
    "assert cross_entropy(_preds_worse, _targets) > cross_entropy(_preds_better, _targets), (\n",
    "    \"wrong loss comparison (cross-entropy)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"596pt\" height=\"78pt\"\n",
       " viewBox=\"0.00 0.00 596.25 78.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 74)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-74 592.25,-74 592.25,4 -4,4\"/>\n",
       "<!-- init -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>init</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">init</text>\n",
       "</g>\n",
       "<!-- predict -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>predict</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"127.51\" cy=\"-18\" rx=\"36.51\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127.51\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">predict</text>\n",
       "</g>\n",
       "<!-- init&#45;&gt;predict -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>init&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.4,-18C62.17,-18 70.91,-18 79.55,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"79.26,-21.5 89.26,-18 79.26,-14.5 79.26,-21.5\"/>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"228.02\" cy=\"-52\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"228.02\" y=\"-46.95\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- predict&#45;&gt;loss -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>predict&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.06,-28.2C168.95,-31.96 181.36,-36.24 192.63,-40.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.4,-43.41 201.99,-43.36 193.68,-36.79 191.4,-43.41\"/>\n",
       "</g>\n",
       "<!-- gradient -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>gradient</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"365.13\" cy=\"-52\" rx=\"41.12\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"365.13\" y=\"-46.95\" font-family=\"Times,serif\" font-size=\"14.00\">gradient</text>\n",
       "</g>\n",
       "<!-- loss&#45;&gt;gradient -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>loss&#45;&gt;gradient</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M255.39,-52C271.56,-52 292.81,-52 312.24,-52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"312.21,-55.5 322.21,-52 312.21,-48.5 312.21,-55.5\"/>\n",
       "</g>\n",
       "<!-- step -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>step</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"470.25\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"470.25\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">step</text>\n",
       "</g>\n",
       "<!-- gradient&#45;&gt;step -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>gradient&#45;&gt;step</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M398.76,-41.24C410.3,-37.44 423.3,-33.15 434.97,-29.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"435.82,-32.71 444.22,-26.25 433.63,-26.06 435.82,-32.71\"/>\n",
       "</g>\n",
       "<!-- step&#45;&gt;predict -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>step&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M443.04,-18C385.63,-18 248.13,-18 175.42,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"175.84,-14.5 165.84,-18 175.84,-21.5 175.84,-14.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"289.52\" y=\"-21.2\" font-family=\"Times,serif\" font-size=\"14.00\">repeat</text>\n",
       "</g>\n",
       "<!-- stop -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>stop</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"561.25\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"561.25\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">stop</text>\n",
       "</g>\n",
       "<!-- step&#45;&gt;stop -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>step&#45;&gt;stop</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M497.47,-18C505.37,-18 514.19,-18 522.67,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"522.39,-21.5 532.39,-18 522.39,-14.5 522.39,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x384d66700>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastbook import gv\n",
    "\n",
    "gv(\"\"\"\n",
    "init->predict->loss->gradient->step->stop\n",
    "step->predict[label=repeat]\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Linear, ReLU, Sequential\n",
    "\n",
    "intermediate_features = 32\n",
    "model = Sequential(\n",
    "    [\n",
    "        Linear(\n",
    "            in_features=IMAGE_SHAPE[0] * IMAGE_SHAPE[1],\n",
    "            out_features=intermediate_features,\n",
    "            init_params_function=init_params,\n",
    "            device=device,\n",
    "        ),\n",
    "        ReLU(device=device),\n",
    "        Linear(\n",
    "            in_features=intermediate_features,\n",
    "            out_features=NUM_CLASSES,\n",
    "            init_params_function=init_params,\n",
    "            device=device,\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time=2.60s, epoch: 0, accuracy: 0.07871093600988388\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 35\u001b[0m\n\u001b[1;32m     24\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     26\u001b[0m learner \u001b[38;5;241m=\u001b[39m Learner(\n\u001b[1;32m     27\u001b[0m     dataloaders\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mcalculate_accuracy,\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch, acc \u001b[38;5;129;01min\u001b[39;00m learner\u001b[38;5;241m.\u001b[39mfit(epochs, learning_rate):\n\u001b[1;32m     36\u001b[0m     elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     37\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "File \u001b[0;32m~/fastbook/exercises/04_exercise_full_mnist/training.py:94\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, epochs, lr)\u001b[0m\n\u001b[1;32m     91\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams(), lr)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[1;32m     95\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_x)\n\u001b[1;32m     96\u001b[0m         preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizer(\n\u001b[1;32m     97\u001b[0m             logits\n\u001b[1;32m     98\u001b[0m         )  \u001b[38;5;66;03m# TODO: is this built into the loss function?\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/fastai/data/load.py:129\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_iter()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idxs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_idxs() \u001b[38;5;66;03m# called in context of main process (not workers/subprocesses)\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m _loaders[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfake_l\u001b[38;5;241m.\u001b[39mnum_workers\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfake_l):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# pin_memory causes tuples to be converted to lists, so convert them back to tuples\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpin_memory \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(b) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m: b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(b)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: b \u001b[38;5;241m=\u001b[39m to_device(b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:42\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/fastai/data/load.py:140\u001b[0m, in \u001b[0;36mDataLoader.create_batches\u001b[0;34m(self, samps)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m    139\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m o:o \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_item, samps))\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunkify(res))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/fastai/data/load.py:185\u001b[0m, in \u001b[0;36mDataLoader.do_batch\u001b[0;34m(self, b)\u001b[0m\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdo_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, b): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretain(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbefore_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, b)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/fastai/data/load.py:181\u001b[0m, in \u001b[0;36mDataLoader.create_batch\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, b): \n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mfa_collate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfa_convert\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprebatched\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprebatched: collate_error(e,b)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/fastai/data/load.py:54\u001b[0m, in \u001b[0;36mfa_collate\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m b \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (default_collate(t) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, _collate_types)\n\u001b[0;32m---> 54\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t[\u001b[38;5;241m0\u001b[39m])([fa_collate(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mt)]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, Sequence)\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m default_collate(t))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/fastai/data/load.py:54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m b \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (default_collate(t) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, _collate_types)\n\u001b[0;32m---> 54\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t[\u001b[38;5;241m0\u001b[39m])([\u001b[43mfa_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mt)]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, Sequence)\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m default_collate(t))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/fastai/data/load.py:53\u001b[0m, in \u001b[0;36mfa_collate\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m b \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, _collate_types)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t[\u001b[38;5;241m0\u001b[39m])([fa_collate(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mt)]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, Sequence)\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m default_collate(t))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fastbook/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from fastai import optimizer\n",
    "from fastai.data.core import DataLoaders\n",
    "from normalize import normalize_log_softmax\n",
    "from torch.nn.functional import cross_entropy\n",
    "from training import Learner, calculate_accuracy, make_optimizer\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Use DataLoaders:\n",
    "data = DataLoaders(\n",
    "    DataLoader(train_dataset, batch_size=256, shuffle=True, device=device),\n",
    "    DataLoader(valid_dataset, batch_size=256, shuffle=False, device=device),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Use full dataset:\n",
    "# data = (train_x, train_y, valid_x, valid_y)\n",
    "\n",
    "accuracies = list()\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "\n",
    "learner = Learner(\n",
    "    dataloaders=data,\n",
    "    model=model,\n",
    "    normalizer=normalize_log_softmax,\n",
    "    opt_func=make_optimizer,\n",
    "    loss_func=cross_entropy,\n",
    "    metrics=calculate_accuracy,\n",
    ")\n",
    "\n",
    "for epoch, acc in learner.fit(epochs, learning_rate):\n",
    "    elapsed = time.time() - start_time\n",
    "    accuracies.append(acc)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Time={elapsed:.2f}s, epoch: {epoch}, accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(optimizer.SGD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import plot_accuracies\n",
    "\n",
    "plot_accuracies(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notify_complete import notify_complete\n",
    "\n",
    "notify_complete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions after all of this:\n",
    "- How to choose the number of intermediate features?\n",
    "- How to play with LRs? Should the LR vary within a training run? With which shape? Is a smaller LR always more conductive to correct learning, even if it takes longer, or is it more likely to get stuck in a local minimum?\n",
    "- What is the effect of batch sizes on learning? What are the trade offs? \n",
    "- How to saturate the GPU? How to profile bottlenecks?\n",
    "- Speaking of which, what's the deal with `DataLoader`? Why so slow? Should I try this on Colab with an nvidia GPU?\n",
    "- How does batching help with learning (not only with speed)?\n",
    "- I'm bruteforcing the epochs. . .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
